== Logging

We will take a look at the available options for logging in ROSA.
As ROSA does not come preconfigured with a logging solution, we can easily set one up.
In this section review the https://docs.openshift.com/dedicated/4/logging/dedicated-cluster-deploying.html#dedicated-cluster-install-deploy[install proceedure] for the EFK (Elasticsearch, Fluentd and Kibana) stack (via Operators), then we will take a look at three methods with which one can view their logs.

. We will look at the logs directly through the pod using `oc logs`.
. We will forward the logs to AWS CloudWatch and view them from there.
. We will use Kibana (EFK Stack) to search our logs.

The cluster logging components are based upon Fluentd, (and Elasticsearch and Kibana, if deployed).
The collector, Fluentd, is deployed to each node in the cluster.
It collects application logs and writes them to Elasticsearch (ES) or forwards it to CloudWatch.
Kibana is the centralized, web UI where users and administrators can create rich visualizations and dashboards with the aggregated data.
We will also look at using AWS CloudWatch as well.

* Learn more about https://docs.openshift.com/container-platform/latest/logging/cluster-logging.html[logging in OpenShift].
* Learn more about the https://docs.openshift.com/rosa/rosa_cluster_admin/rosa_logging/rosa-install-logging.html[logging add-on service].

=== Installing the Cluster Logging Add-on service

!!!
danger 	If you plan on running EFK +++<u>+++do not follow+++</u>+++ the installation steps in this section but rather follow the https://docs.openshift.com/container-platform/latest/logging/cluster-logging-deploying.html[Installing OpenShift Logging] steps and skip down to <<view-logs-with-kibana,View logs with Kibana>>.

In the following steps we will install the logging add-on service to forward our logs;
in our case to CloudWatch.
If you did not follow the "Getting Started" guide of this workshop and *did not* install ROSA with STS, then you can skip to install the service though the OCM UI or by using the CLI (in step 8).
Otherwise, there are a few steps we need to do first in order to get this to work for ROSA with STS.

!!!
note 	These steps were adopted from our Managed OpenShift Black Belts https://mobb.ninja/docs/rosa/sts-cluster-logging-addon/[here].

. Create a IAM Trust Policy document.
+
----
 cat << EOF > /tmp/trust-policy.json
 {
     "Version": "2012-10-17",
     "Statement": [
         {
             "Effect": "Allow",
             "Action": [
                 "logs:CreateLogGroup",
                 "logs:CreateLogStream",
                 "logs:DescribeLogGroups",
                 "logs:DescribeLogStreams",
                 "logs:PutLogEvents",
                 "logs:GetLogEvents",
                 "logs:PutRetentionPolicy",
                 "logs:GetLogRecord"
             ],
             "Resource": "arn:aws:logs:*:*:*"
         }
     ]
 }
 EOF
----

. Create IAM Policy
+
----
 POLICY_ARN=$(aws iam create-policy --policy-name "RosaCloudWatchAddon" --policy-document file:///tmp/trust-policy.json --query Policy.Arn --output text)
 echo $POLICY_ARN
----

. Create service account
+
----
 aws iam create-user --user-name RosaCloudWatchAddon --query User.Arn --output text
----

. Attach policy to user
+
----
 aws iam attach-user-policy --user-name RosaCloudWatchAddon --policy-arn ${POLICY_ARN}
----

. Create AccessKeyId and SecretAccessKey
+
----
 aws iam create-access-key --user-name RosaCloudWatchAddon
----

. Save the output to the following environment variables
+
----
 export AWS_ID=<from above>
 export AWS_KEY=<from above>
----

. Create a secret for the addon to use
+
----
 cat << EOF | kubectl apply -f -
 apiVersion: v1
 kind: Secret
 metadata:
     name: instance
     namespace: openshift-logging
 stringData:
     aws_access_key_id: ${AWS_ID}
     aws_secret_access_key: ${AWS_KEY}
 EOF
----

. Access the https://console.redhat.com/OpenShift[OCM UI], select your cluster, and click on the *Add-ons* tab.
. Click on the *Cluster Logging Operator*
+
image::images/9-ostoy-logadd.png[Logging addon]

. Click _Install_
. Select the logs you want to collect.
If you want to forward somewhere other than CloudWatch leave that box unchecked.
You can select the defaults and leave the region blank (unless you want to use a different region).
Click _Install_.
+
image::images/9-ostoy-sellog.png[select logs]

. It will take about 10 minutes to install.

=== Output data to the streams/logs

. Output a message to _stdout_ Click on the _Home_ menu item and then click in the message box for "Log Message (stdout)" and write any message you want to output to the _stdout_ stream.
You can try "*All is well!*".
Then click "Send Message".
+
image::images/9-ostoy-stdout.png[Logging stdout]

. Output a message to _stderr_ Click in the message box for "Log Message (stderr)" and write any message you want to output to the _stderr_ stream.
You can try "*Oh no!
Error!*".
Then click "Send Message".
+
image::images/9-ostoy-stderr.png[Logging stderr]

=== View application logs using `oc`

. Go to the CLI and enter the following command to retrieve the name of your frontend pod which we will use to view the pod logs:
+
----
 $ oc get pods -o name
 pod/ostoy-frontend-679cb85695-5cn7x
 pod/ostoy-microservice-86b4c6f559-p594d
----

So the pod name in this case is *ostoy-frontend-679cb85695-5cn7x*.

. Run `oc logs ostoy-frontend-679cb85695-5cn7x` and you should see your messages:
+
----
 $ oc logs ostoy-frontend-679cb85695-5cn7x
 [...]
 ostoy-frontend-679cb85695-5cn7x: server starting on port 8080
 Redirecting to /home
 stdout: All is well!
 stderr: Oh no! Error!
----

You should see both the _stdout_ and _stderr_ messages.

=== View logs with CloudWatch

. Access the web console for your AWS account and go to CloudWatch.
. Click on _Logs_ > _Log groups_ in the left menu to see the different groups of logs depending on what you selected during installation.
If you followed the previous steps you should see 2 groups.
One for `<cluster-name>-XXXXX-application` and one for `<cluster-name>-XXXXX-infrastructure`.
+
image::images/9-cw.png[cloudwatch]

. Click on `<cluster-name>-XXXXX.application`
. Click on the log stream for the "frontend" pod.
It will be titled something like `+kubernetes.var[...]ostoy-frontend-[...]+`
+
image::images/9-logstream.png[cloudwatch2]

. Filter for "stdout" and "stderr" the expand the row to show the message we had entered earlier along with much other information.
+
image::images/9-stderr.png[cloudwatch2]

. We can also see other messages in our logs from the app.
Enter "microservice" in the search bar, and expand one of the entries.
This shows us the color recieved from the microservice and which pod sent that color to our frontend pod.
+
image::images/9-messages.png[messages]

You can also use some of the other features of CloudWatch to obtain useful information.
But https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html[how to use CloudWatch] is beyond the scope of this tutorial.

=== View logs with Kibana

!!!
note  		In order to use EFK, this section assumes that you have successfully completed the steps outlined in https://docs.openshift.com/container-platform/latest/logging/cluster-logging-deploying.html[Installing OpenShift Logging].

. Run the following command to get the route for the Kibana console:

  oc get route -n openshift-logging

. Open up a new browser tab and paste the URL.
You will first have to define index patterns.
Please see the https://docs.openshift.com/container-platform/latest/logging/cluster-logging-deploying.html#cluster-logging-visualizer-indices_cluster-logging-deploying[Defining Kibana index patterns] section of the documentation for further instructions on doing so.
+
// ![Kibana console](images/9-kibana.png)

==== Familiarization with the data

In the main part of the console you should see three entries.
These will contain what we saw in the above section (viewing through the pods).
You will see the _stdout_ and _stderr_ messages that we inputted earlier (though you may not see it right away as we might have to filter for it).
In addition to the log output you will see information about each entry.
You can see things like:

* namespace name
* pod name
* host ip address
* timestamp
* log level
* message

image::images/9-logoutput.png[Kibana data]

You will also see that there is data from multiple sources and multiple messages.
If we expand one of the twisty-ties we can see further details

image::images/9-logdata.png[log data]

==== Filtering Results

Let's look for any errors encountered in our app.
Since we have many log entries (most from the previous networking section) we may need to filter to make it easier to find the errors.
To find the error message we outputted to _stderr_ lets create a filter.

* Click on "Add a filter+" under the search bar on the upper left.
* For "Fields..." select (or type) "level"
* For "Operators" select "is"
* In "Value..." type in "err"
* Click "Save"

image::images/9-filtererr.png[Expand data]

You should see now only one row is returned that contains our error message.

image::images/9-erronly.png[Expand data]

!!!
note 	If nothing is returned, depending on how much time has elapsed since you've outputted the messages to the _stdout_ and _stderr_ streams you may need to set the proper time frame for the filter.
If you are following this lab consistently then the default should be fine.
Otherwise, in the Kibana console, click on the top right where it should say "Last 15 minutes" and click on "Quick" then "Last 1 hour" (though adjust to your situation as needed).
