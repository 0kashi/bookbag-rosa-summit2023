== Introduction

When deploying your ROSA cluster, you can configure many aspects of your worker nodes, but what happens when you need to modify your cluster after it has already been created?
These activities include scaling the number of nodes, changing the instance type, adding labels or taints, just to name a few.

Many of these changes are done using machine pools.
machine pools ensure that a specified number of Machine replicas are running at any given time.
Think of a machine pool as a "template" for the kinds of Machines that make up the worker nodes of your cluster.
If you'd like to learn more, see https://docs.openshift.com/rosa/rosa_cluster_admin/rosa_nodes/rosa-nodes-machinepools-about.html[About machine pools].

== Scaling worker nodes

. In ROSA worker nodes are managed using machine pools. Let's see what machine pools already exist in our cluster.
To do so, run the following command:
+
[source,sh,role=execute]
----
rosa list machinepools -c rosa-${GUID}
----
+
.Sample Output
[source,text,options=nowrap]
----
ID       AUTOSCALING  DESIRED REPLICAS  CURRENT REPLICAS  INSTANCE TYPE  LABELS    TAINTS    AVAILABILITY ZONE  SUBNET                    VERSION  AUTOREPAIR  TUNING CONFIGS  MESSAGE
workers  No           2                 2                 m5.xlarge                          us-east-2a         subnet-07465e0e286d4a171  4.12.15  Yes
----
+
Note that there is only a machine pool for worker nodes.

. Now that we know that we have two worker nodes, let's create a machine pool to add a new worker node using the ROSA CLI. Let's create this one of a different instance type.  We already have `m5.xlarge`, so for this machine pool let's use `m6a.xlarge`.  See https://docs.openshift.com/rosa/rosa_architecture/rosa_policy_service_definition/rosa-service-definition.html#rosa-sdpolicy-aws-instance-types_rosa-service-definition[AWS Instance Types] for all supported instance types.
+
To do so, run the following command:
+
[source,sh,role=execute]
----
rosa create machinepool -c rosa-${GUID} --replicas 1 --name workshop --instance-type m6a.xlarge
----
+
.Sample Output
[source,text,options=nowrap]
----
I: Fetching instance types
I: Machine pool 'workshop' created successfully on cluster 'rosa-6n4s8'
I: To view all machine pools, run 'rosa list machinepools -c rosa-6n4s8'
----
+
This command adds a single m6a.xlarge instance to the availability zone. It will take a few minutes for the node to become available.

. Confirm the machine pool was created and that the node is available.
+
[source,sh,role=execute]
----
rosa list machinepools -c rose-${GUID}
----
+
.Sample Output
[source,text,options=nowrap]
----
ID        AUTOSCALING  DESIRED REPLICAS  CURRENT REPLICAS  INSTANCE TYPE  LABELS    TAINTS    AVAILABILITY ZONE  SUBNET                    VERSION  AUTOREPAIR  TUNING CONFIGS  MESSAGE
workers   No           2                 2                 m5.xlarge                          us-east-2a         subnet-07465e0e286d4a171  4.12.15  Yes
workshop  No           1                 1                 m6a.xlarge                         us-east-2a         subnet-07465e0e286d4a171  4.12.15  Yes
----

. Now, let's scale up our selected machine pool from one to two machines.
To do so, run the following command:
+
[source,sh,role=execute]
----
rosa edit machinepool -c rosa-${GUID} --replicas 2 workshop
----
+
.Sample Output
[source,text,options=nowrap]
----
I: Updated machine pool 'workshop' on cluster 'rosa-6n4s8'
----
+ After a few minutes we should see 2 nodes in the machine pool.

. We don't actually need these extra worker nodes so let's scale the cluster back down to a total of 2 worker nodes by deleting the "workshop" machine pool.
To do so, run the following command:
+
[source,sh,role=execute]
----
rosa delete machinepool -c rosa-${GUID} workshop --yes
----
+
.Sample Output
[source,text,options=nowrap]
----
I: Successfully deleted machine pool 'workshop' from cluster 'rosa-6n4s8'
----

As you've seen you can create machine pools as needed even of different instance types. You can also use machine pools to configure node labels and autoscaling.

You've successfully scaled your cluster up and back down to two worker nodes.
